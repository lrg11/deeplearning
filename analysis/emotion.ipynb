{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print'. Did you mean print(...)? (1316064739.py, line 77)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 77\u001b[1;36m\u001b[0m\n\u001b[1;33m    print train_vecs.shape\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m Missing parentheses in call to 'print'. Did you mean print(...)?\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "# 用gensim去做word2vec的处理，用sklearn当中的SVM进行建模\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jieba\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.svm import SVC\n",
    "import sys  \n",
    "reload(sys)  \n",
    "sys.setdefaultencoding('utf8')\n",
    "\n",
    "#  载入数据，做预处理(分词)，切分训练集与测试集\n",
    "def load_file_and_preprocessing():\n",
    "    neg=pd.read_excel('data/neg.xls',header=None,index=None)\n",
    "    pos=pd.read_excel('data/pos.xls',header=None,index=None)\n",
    "\n",
    "    cw = lambda x: list(jieba.cut(x))\n",
    "\n",
    "    # 新增一列 word ,存放分好词的评论，pos[0]代表表格第一列\n",
    "\n",
    "    pos['words'] = pos[0].apply(cw)\n",
    "    neg['words'] = neg[0].apply(cw)\n",
    "\n",
    "    # np.ones(len(pos)) 新建一个长度为len(pos)的数组并初始化元素全为1来标注好评\n",
    "    # np.concatenate（）连接数组\n",
    "    # axis=0 向下执行方法 axis=1向右执行方法\n",
    "    y = np.concatenate((np.ones(len(pos)), np.zeros(len(neg))),axis=0)\n",
    "\n",
    "    # train_test_split：从样本中随机的按比例选取train data和testdata\n",
    "    # 一般形式：train_test_split(train_data,train_target,test_size=0.4, random_state=0)\n",
    "    # train_data：所要划分的样本特征集\n",
    "    # train_target：所要划分的样本结果（标注）\n",
    "    # test_size：样本占比，如果是整数的话就是样本的数量\n",
    "    # random_state：是随机数的种子。\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(np.concatenate((pos['words'], neg['words'])), y, test_size=0.2)\n",
    "    \n",
    "    np.save('svm_data/y_train.npy',y_train)\n",
    "    np.save('svm_data/y_test.npy',y_test)\n",
    "    return x_train,x_test\n",
    "\n",
    "\n",
    "\n",
    "# 对每个句子的所有词向量取均值，来生成一个句子的vector\n",
    "def build_sentence_vector(text, size,imdb_w2v):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in text:\n",
    "        try:\n",
    "            vec += imdb_w2v[word].reshape((1, size))\n",
    "            count += 1.\n",
    "        except KeyError:\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec\n",
    "\n",
    "# 计算词向量\n",
    "def get_train_vecs(x_train, x_test):\n",
    "    n_dim = 300\n",
    "    # 初始化模型和词表\n",
    "    imdb_w2v = Word2Vec(x_train, size=n_dim, min_count=10)\n",
    "    # imdb_w2v = Word2Vec(size=300, window=5, min_count=10, workers=12)\n",
    "    # imdb_w2v.build_vocab(x_train)\n",
    "    #\n",
    "    # imdb_w2v.train(x_train,\n",
    "    #                total_examples=imdb_w2v.corpus_count,\n",
    "    #                epochs=imdb_w2v.iter)\n",
    "\n",
    "\n",
    "    train_vecs = np.concatenate([build_sentence_vector(z, n_dim, imdb_w2v) for z in x_train])\n",
    "    # train_vecs = scale(train_vecs)\n",
    "\n",
    "    np.save('svm_data/train_vecs.npy', train_vecs)\n",
    "    print train_vecs.shape\n",
    "    # 在测试集上训练\n",
    "    imdb_w2v.train(x_test)\n",
    "    # imdb_w2v.train(x_test,\n",
    "    #                total_examples=imdb_w2v.corpus_count,\n",
    "    #                epochs=imdb_w2v.iter)\n",
    "\n",
    "    imdb_w2v.save('svm_data/w2v_model/w2v_model.pkl')\n",
    "    # Build test tweet vectors then scale\n",
    "    test_vecs = np.concatenate([build_sentence_vector(z, n_dim, imdb_w2v) for z in x_test])\n",
    "    # test_vecs = scale(test_vecs)\n",
    "    np.save('svm_data/test_vecs.npy', test_vecs)\n",
    "    print test_vecs.shape\n",
    "\n",
    "\n",
    "\n",
    "def get_data():\n",
    "    train_vecs=np.load('svm_data/train_vecs.npy')\n",
    "    y_train=np.load('svm_data/y_train.npy')\n",
    "    test_vecs=np.load('svm_data/test_vecs.npy')\n",
    "    y_test=np.load('svm_data/y_test.npy')\n",
    "    return train_vecs,y_train,test_vecs,y_test\n",
    "\n",
    "# 训练svm模型\n",
    "\n",
    "def svm_train(train_vecs,y_train,test_vecs,y_test):\n",
    "    clf=SVC(kernel='rbf',verbose=True)\n",
    "    clf.fit(train_vecs,y_train)\n",
    "    joblib.dump(clf, 'svm_data/svm_model/model.pkl')\n",
    "    print clf.score(test_vecs,y_test)\n",
    "\n",
    "\n",
    "# 构建待预测句子的向量\n",
    "\n",
    "def get_predict_vecs(words):\n",
    "    n_dim = 300\n",
    "    imdb_w2v = Word2Vec.load('svm_data/w2v_model/w2v_model.pkl')\n",
    "    #imdb_w2v.train(words)\n",
    "    train_vecs = build_sentence_vector(words, n_dim,imdb_w2v)\n",
    "    #print train_vecs.shape\n",
    "    return train_vecs\n",
    "\n",
    "# 对单个句子进行情感判断\n",
    "\n",
    "def svm_predict(string):\n",
    "    words=jieba.lcut(string)\n",
    "    words_vecs=get_predict_vecs(words)\n",
    "    clf=joblib.load('svm_data/svm_model/model.pkl')\n",
    "     \n",
    "    result=clf.predict(words_vecs)\n",
    "    \n",
    "    if int(result[0])==1:\n",
    "        print string,' positive'\n",
    "    else:\n",
    "        print string,' negative'\n",
    "\n",
    "#\n",
    "#x_train,x_test = load_file_and_preprocessing()\n",
    "#get_train_vecs(x_train,x_test)\n",
    "#train_vecs,y_train,test_vecs,y_test = get_data()\n",
    "svm_train(train_vecs,y_train,test_vecs,y_test)\n",
    "\n",
    "##对输入句子情感进行判断\n",
    "#string='电池充完了电连手机都打不开.简直烂的要命.真是金玉其外,败絮其中!连5号电池都不如'\n",
    "#string='这手机真棒，从1米高的地方摔下去就坏了'\n",
    "#svm_predict(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
